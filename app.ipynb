{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = './data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 208574 characters.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            all_text += doc.page_content + \"\\n\" \n",
    "\n",
    "print(f\"Total text length: {len(all_text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 690\n",
      "Sample chunk: Large Language Models: A Survey\n",
      "Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\n",
      "Richard Socher, Xavier Amatriain, Jianfeng Gao\n",
      "Abstract—Large Language Models (LLMs) have drawn a\n",
      "lot of attention due to their strong performance on a wide\n",
      "range of natural language tasks, since the release of ChatGPT\n",
      "in November 2022. LLMs’ ability of general-purpose language\n",
      "understanding and generation is acquired by training billions of\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "print(f\"Sample chunk: {chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma.from_documents(docs, embedding=embedding, persist_directory=\"./chroma_db/RRF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db3 = Chroma(persist_directory=\"./chroma_db/RRF\", embedding_function=embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db3.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually handle Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "question = \"What is Search types discuss,only give types?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "# manually add in chathistory\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Search types discuss,only give types?',\n",
       " 'chat_history': [HumanMessage(content='What is Search types discuss,only give types?'),\n",
       "  AIMessage(content=\"According to the provided context, search types discussed are:\\n\\n* Google search engine\\n* Retrieval augmented generation (RAG)\\n\\nI don't know any other specific search types mentioned in this context.\")],\n",
       " 'context': [Document(page_content='the Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5 search results, and annotates a\\nlong answer (typically a paragraph) and a short answer'),\n",
       "  Document(page_content='C. Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a'),\n",
       "  Document(page_content='TABLE I: High-level Overview of Popular Language Models\\nType\\nModel Name\\n#Parameters\\nRelease\\nBase Models\\nOpen\\nSource\\n#Tokens\\nTraining dataset\\nBERT\\n110M, 340M\\n2018\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nRoBERTa\\n355M\\n2019\\n-\\n✓\\n2.2T\\nBooksCorpus,\\nEnglish\\nWikipedia,\\nCC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-Only\\nALBERT\\n12M,\\n18M,\\n60M,\\n235M\\n2019\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nDeBERTa\\n-\\n2020\\n-\\n✓\\n-\\nBooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet\\n110M, 340M\\n2019')],\n",
       " 'answer': \"According to the provided context, search types discussed are:\\n\\n* Google search engine\\n* Retrieval augmented generation (RAG)\\n\\nI don't know any other specific search types mentioned in this context.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"Explain any of above type you mentioned?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "# manually add in chathistory\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Explain any of above type you mentioned?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='psychological parlance, has been appropriated within the field\\nof artificial intelligence.\\nHallucinations in LLMs can be broadly categorized into\\ntwo types:\\n1)\\nIntrinsic Hallucinations: These directly conflict with\\nthe source material, introducing factual inaccuracies\\nor logical inconsistencies.\\n2)\\nExtrinsic Hallucinations: These, while not contra-\\ndicting, are unverifiable against the source, encom-\\npassing speculative or unconfirmable elements.'),\n",
       "  Document(page_content='reasoning.\\n2)\\nManual CoT: A more complex variant, it requires\\nproviding step-by-step reasoning examples as tem-\\nplates for the model. While yielding more effective\\nresults, it poses challenges in scalability and mainte-\\nnance.\\nManual CoT is more effective than zero-shot. However,\\nthe effectiveness of this example-based CoT depends on the\\nchoice of diverse examples, and constructing prompts with\\nsuch examples of step by step reasoning by hand is hard and'),\n",
       "  Document(page_content='This method allows the LLM to explore various possibilities\\nand hypotheses, much like human cognitive processes where\\nmultiple scenarios are considered before determining the most\\nlikely one.\\nA critical aspect of ToT is the evaluation of these reasoning\\npaths. As the LLM generates different branches of thought,\\neach is assessed for its validity and relevance to the query.\\nThis process involves real-time analysis and comparison of\\nthe branches, leading to a selection of the most coherent and')],\n",
       " 'answer': \"I'd be happy to explain! Extrinsic Hallucinations in LLMs refer to unverifiable or speculative elements that are not necessarily contradictory to the source material. These hallucinations can encompass hypothetical or uncertain information, which is similar to how humans consider multiple scenarios before determining a most likely one through reasoning processes like Top-Down (ToT).\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"my name is Mustafa . can you Explain any LLM?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'my name is Mustafa . can you Explain any LLM?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='LLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with\\ndynamic environment. Therefore, LLM-based agents often\\nneed to augment LLMs to e.g., obtain updated information\\nfrom external knowledge bases, verify whether a system action\\nproduces the expected result, and cope with when things do\\nnot go as expected, etc. We will discuss in detail LLM-based\\nagents in Section IV.'),\n",
       "  Document(page_content='but also be used to augment the capabilities of LLMs going\\nas far as turning an LLM into a full-blown AI agent with the\\nability to interface with the external world.\\nA. LLM limitations\\nIt is important to remember that LLMs are trained to predict\\na token. While fine-tuning and alignment improves their per-\\nformance and adds different dimensions to their abilities, there\\nare still some important limitations that come up, particularly\\nif they are used naively. Some of them include the following:\\n•'),\n",
       "  Document(page_content='Through advanced usage and augmentation techniques,\\nLLMs can be deployed as so-called AI agents: artificial entities\\nthat sense their environment, make decisions, and take actions.\\nPrevious research has focused on developing agents for specific\\ntasks and domains. The emergent abilities demonstrated by\\nLLMs make it possible to build general-purpose AI agents\\nbased on LLMs. While LLMs are trained to produce responses\\nin static settings, AI agents need to take actions to interact with')],\n",
       " 'answer': \"Hello Mustafa! An LLM (Large Language Model) is a type of artificial intelligence that's trained on vast amounts of text data. It can generate responses based on what it was trained on, but it has limitations as it's designed to predict a token rather than interact with the external world in real-time.\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"what is my name?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is my name?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(page_content='7B, 13B, 34B, 70B\\n2023\\n-\\n✓\\n2T\\nOnline sources\\nAlpaca\\n7B\\n2023\\nLLaMA1\\n✓\\n-\\nGPT-3.5\\nVicuna-13B\\n13B\\n2023\\nLLaMA1\\n✓\\n-\\nGPT-3.5\\nLLaMA Family\\nKoala\\n13B\\n2023\\nLLaMA\\n✓\\n-\\nDialogue data\\nMistral-7B\\n7.3B\\n2023\\n✓\\n-\\n-\\nCode Llama\\n34\\n2023\\nLLaMA2\\n✓\\n500B\\nPublicly available code\\nLongLLaMA\\n3B, 7B\\n2023\\nOpenLLaMA\\n✓\\n1T\\n-\\nLLaMA-Pro-8B\\n8.3B\\n2024\\nLLaMA2-7B\\n✓\\n80B\\nCode and math corpora\\nTinyLlama-1.1B\\n1.1B\\n2024\\nLLaMA1.1B\\n✓\\n3T\\nSlimPajama, Starcoderdata\\nPaLM\\n8B, 62B, 540B\\n2022\\n-\\n×\\n780B'),\n",
       "  Document(page_content='assistance from ChatGPT.\\nStarCoder: In [97], Li et al. introduced StarCoder and\\nStarCoderBase. They are 15.5B parameter models with 8K\\ncontext length, infilling capabilities and fast large-batch in-\\nference enabled by multi-query attention. StarCoderBase is\\ntrained on one trillion tokens sourced from The Stack, a\\nlarge collection of permissively licensed GitHub repositories\\nwith inspection tools and an opt-out process. They fine-tuned'),\n",
       "  Document(page_content='combinations of LLMs, human inputs, and tools.\\nBabyAGI [235] is an autonomous Artificial Intelligence\\nagent, that is designed to generate and execute tasks based on\\ngiven objectives. It harnesses cutting-edge technologies from\\nOpenAI, Pinecone, LangChain, and Chroma to automate tasks\\nand achieve specific goals. In this blog post, we will dive\\ninto the unique features of BabyAGI and explore how it can\\nstreamline task automation.\\nC. Prompting Libraries')],\n",
       " 'answer': 'I don\\'t know the answer to your question about \"what is my name?\" as I\\'m an assistant for question-answering tasks, not a personal naming service.'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
