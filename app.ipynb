{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import RetrievalQA\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_folder_path = './data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total text length: 208574 characters.\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(pdf_folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        pdf_path = os.path.join(pdf_folder_path, filename)\n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        docs = loader.load()\n",
    "        for doc in docs:\n",
    "            all_text += doc.page_content + \"\\n\" \n",
    "\n",
    "print(f\"Total text length: {len(all_text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(all_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 690\n",
      "Sample chunk: Large Language Models: A Survey\n",
      "Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu\n",
      "Richard Socher, Xavier Amatriain, Jianfeng Gao\n",
      "Abstract—Large Language Models (LLMs) have drawn a\n",
      "lot of attention due to their strong performance on a wide\n",
      "range of natural language tasks, since the release of ChatGPT\n",
      "in November 2022. LLMs’ ability of general-purpose language\n",
      "understanding and generation is acquired by training billions of\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of chunks created: {len(chunks)}\")\n",
    "print(f\"Sample chunk: {chunks[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [Document(page_content=chunk) for chunk in chunks]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db2 = Chroma.from_documents(docs, embedding=embedding, persist_directory=\"./chroma_db/RRF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\musta\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "db3 = Chroma(persist_directory=\"./chroma_db/RRF\", embedding_function=embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db3.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm, retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually handle Chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "question = \"What is Search types discuss,only give types?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "# manually add in chathistory\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_1[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Search types discuss,only give types?',\n",
       " 'chat_history': [HumanMessage(content='What is Search types discuss,only give types?'),\n",
       "  AIMessage(content=\"According to the provided context, search types discussed are:\\n\\n* Google search engine\\n* Retrieval augmented generation (RAG)\\n\\nI don't know any other specific search types mentioned in this context.\")],\n",
       " 'context': [Document(page_content='the Google search engine as questions. An annotator\\nis presented with a question along with a Wikipedia\\npage from the top 5 search results, and annotates a\\nlong answer (typically a paragraph) and a short answer'),\n",
       "  Document(page_content='C. Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a'),\n",
       "  Document(page_content='TABLE I: High-level Overview of Popular Language Models\\nType\\nModel Name\\n#Parameters\\nRelease\\nBase Models\\nOpen\\nSource\\n#Tokens\\nTraining dataset\\nBERT\\n110M, 340M\\n2018\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nRoBERTa\\n355M\\n2019\\n-\\n✓\\n2.2T\\nBooksCorpus,\\nEnglish\\nWikipedia,\\nCC-NEWS,\\nSTORIES (a subset of Common Crawl), Reddit\\nEncoder-Only\\nALBERT\\n12M,\\n18M,\\n60M,\\n235M\\n2019\\n-\\n✓\\n137B\\nBooksCorpus, English Wikipedia\\nDeBERTa\\n-\\n2020\\n-\\n✓\\n-\\nBooksCorpus, English Wikipedia, STORIES, Red-\\ndit content\\nXLNet\\n110M, 340M\\n2019')],\n",
       " 'answer': \"According to the provided context, search types discussed are:\\n\\n* Google search engine\\n* Retrieval augmented generation (RAG)\\n\\nI don't know any other specific search types mentioned in this context.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_question = \"Explain any of above type you mentioned?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "# manually add in chathistory\n",
    "\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=question),\n",
    "        AIMessage(content=ai_msg_2[\"answer\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Explain any of above type you mentioned?',\n",
       " 'chat_history': [HumanMessage(content='What is Search types discuss,only give types?'),\n",
       "  AIMessage(content=\"According to the provided context, search types discussed are:\\n\\n* Google search engine\\n* Retrieval augmented generation (RAG)\\n\\nI don't know any other specific search types mentioned in this context.\"),\n",
       "  HumanMessage(content='What is Search types discuss,only give types?'),\n",
       "  AIMessage(content=\"The two search types mentioned in the context are:\\n\\n1. A search engine (no specific one is mentioned, but Google is an example)\\n2. Retrieval augmented generation (RAG)\\n\\nThere's no specific explanation provided for these search types in this context.\")],\n",
       " 'context': [Document(page_content='for the model to generate the final response. A RAG system\\nincludes three important components: Retrieval, Generation,\\nAugmentation [165].\\na) RAG-aware prompting techniques: Because of the\\nimportance of RAG to build advanced LLM systems, several\\nRAG-aware prompting techniques have been developed re-\\ncently. One such technique is Forward-looking Active Retrieval\\nAugmented Generation (FLARE)\\nForward-looking Active Retrieval Augmented Generation'),\n",
       "  Document(page_content='For more details on RAG framework and its relevant works,\\nwe refer the readers to this survey of retrieval augmented\\ngenerations [165].\\nD. Using External Tools\\nRetrieving information from an external knowledge source\\nas described above is only one of the potential ways to augment\\nan LLM. More generally, an LLM can access any number\\nof external tools (e.g. an API to a service) to augment its\\nfunctionality. In that regards, RAG can be seen as a specific'),\n",
       "  Document(page_content='C. Augmenting LLMs through external knowledge - RAG\\nOne of the main limitations of pre-trained LLMs is their\\nlack of up-to-date knowledge or access to private or use-\\ncase-specific information. This is where retrieval augmented\\ngeneration (RAG) comes into the picture [164]. RAG, illus-\\ntrated in figure 37, involves extracting a query from the input\\nprompt and using that query to retrieve relevant information\\nfrom an external knowledge source (e.g. a search engine or a')],\n",
       " 'answer': \"The two search types mentioned in the context are:\\n\\n1. A search engine (no specific one is mentioned, but Google is an example)\\n2. Retrieval augmented generation (RAG)\\n\\nThere's no specific explanation provided for these search types in this context.\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_msg_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To fine-tune an LLM, you can use instruction tuning, which involves providing specific prompts that align with the expectations humans would have when giving instructions. This approach helps improve the model's performance by scaling laws and has been shown to outperform original foundation models in many cases. The specific approach and datasets used for instruction tuning may vary, but generally speaking, it leads to better results.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"my nme is sosa . i have one question about : How i fine tune LLM?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  \n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval-Augmentation-Generation) is a system that includes three components: Retrieval, Generation, and Augmentation. The key difference between RAG and instruction tuning is that RAG involves retrieving relevant information from an external knowledge source, adding it to the original prompt, and then generating a response, whereas instruction tuning focuses on providing specific prompts that align with human expectations for task completion.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"diffrenece between Rag and above texhnique?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  \n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
